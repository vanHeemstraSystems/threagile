# 400 - Threagile Output

Transcript from [Video](https://youtu.be/5n-8LqHMoJ0?t=453):

Risk Output
7:33
so what was generated from the execution is basically some kind of diagrams more
7:40
on that later so we see for example a data flow diagram that has been automatically laid out
7:46
and the colors are basically due to the different data flows and different risk ratings in
7:53
terms of sensitivity of the data and also in terms of the places where custom code is
8:00
developed or not whether it's on the internet or not or out of scope or not so that's a little bit of a scheme
8:06
behind that layout and it has been automatically generated also we do have a pdf report that has
PDF Report
8:13
been generated that's um a little bit long because it's in terms of reading density giving two different
8:20
entries for viewing the risks that have been identified according to the risk rules
8:26
and also some kind of documentation character so it's a little bit of lengthier and we do have in that kind of
8:33
report some management summary and impact analysis of some pre-risk mitigation after the
8:41
risk mitigation state risks and the diagrams are also inside
8:46
same is for some kind of other documentation and most importantly we do have the
8:51
risks that have been identified including individual risks more on that later and the color
8:56
again in terms of criticality and we do have those risks
9:01
as entry points and we do have the technical assets also as entry points that basically is
9:07
the other view of the same data and we have a data loss probability that has also been generated by each
9:14
data asset and a little bit more of documentation so basically some management summary as
9:19
expected you can even customize that with some custom text and some distribution diagrams
9:27
and an impact analysis of the risks 83 risks initially identified in 27
9:32
categories of that example file including some individual risk a critical one so even custom risks
9:39
can be added to that kind of scheme and you can click on any of those to get to more details
9:45
but more on that later and we do have a risk mitigation where we see a distribution of the
9:52
initial identified risks and the remaining ones so basically some have been mitigated
9:57
some have been in progress or many here are unchecked which is not good
10:02
and the impact analysis of the remaining ones so after that kind of presented mitigation that is also maintainable
10:09
inside the yaml file also we do have some kind of embedding of the
10:14
diagrams and a little bit of more documentation stuff not going into details here we've got a stride distribution so that
10:21
you see those risks even here it's clickable to go to the details accordingly the spoofing tampering and
10:28
to the other categories like repudiation information disclosure denial of service and elevation of
10:34
privilege within stride again different views same data you have an assignment by function so that gives
10:41
who is most probably are responsible for mitigating the risk
10:48
business side architecture side development side or operation side again clickable
10:55
and an analysis of the relative attacker attractiveness that's something i'm not going to cover
11:00
in that short video here it's a algorithm that's calculating how attacker attractive
11:06
the different elements in the diagram or in the threat model are accordingly due to some graph
11:12
calculation a data mapping where we see some kind of data assets on the left the data assets
11:18
on the left where they are stored and processed on which elements on the right and here everything is red on the left
11:23
side because they are either touching a a right on the right side a red or higher rated
11:30
data asset so you see when you basically downgrade some risks here by mitigating them properly
11:36
then you would have high impact on the left side getting the color from red to amber or even better and a little bit more
11:44
documentation and then each risk has some kind of basically some kind of
11:51
initial page where you see an overview that includes a description the impact
11:56
a detection logic and what could be false positives and the mitigation and that mitigation
12:02
includes links directly to the o wasp cheat sheet and the o wasp asvs chapter so that things can
12:10
be read up from there as well and every risk here is rated
12:15
according to individual rules and also has some description here this was
12:21
identified one time again an xml risk was identified one time here and we do have some crosstalk
12:27
scripting risks that potential cross-scripting risks that have been identified four times
12:33
so we have them each here you can click on any of that to go to the technical asset view
12:38
or the shortcut here just to go back clicking on that to the table of contents for easier navigation
12:46
so let's go into the risks by technical asset for example we can go to the back office
12:52
erp system it has 14 remaining out of 18 risks click on that and here we see the risks being here
12:59
rated high risk elevated medium and for example the xml risk is
13:05
here and matching at this back office erp system or some um untrusted deserialization
13:12
vulnerability or some medium risks here like crosshatch scripting across a request forgery
13:18
or unencrypted communications and stuff like that and a little bit of documentation
13:23
that's just basically for the documentation folks you can click on any of those risks so for example we can click on let's say
13:31
here the um s csrf risk and then we've got crosshat
13:36
requests foundry explained and where it matched seven times where it's matching so we have those seven times
13:42
here as well and finally we do have some kind of data loss probabilities
13:49
here that's also calculated everything is red because we didn't mitigate too many risks yet and for example the
13:57
customer accounts are read due to 39 unmitigated risks
14:02
still out of 57 identified i can click on that and i see where it's processed on which
14:09
technical assets where it's sent or received via which connection links and here i see
14:15
its data losses probable due to zooming in having 39 remaining
14:21
risks those are the remaining ones and each one has a probable or possible or improbable
14:28
style of that's the risk id here a style of weather that kind of risk is
14:35
risking that you lose data so some injection obviously might definitely have some risk of losing data crossover
14:42
scripting possible depends on encrypted communication also possible needs to be a network-based attack
14:49
happening and some other ones is just more improbable so i can click on any of those and let's
14:55
say missing cloud hardening click and then i've got some missing cloud hardening information here
15:00
so it's very easy to navigate throughout these kinds of structures also we do have
15:08
some more assets so we do have a would also generate it easier to handle excel file that i'm
15:15
going to import it's just the same data different view and it's
15:20
giving me a sortable and easier filterable kind of documentation of what risk
15:28
has been identified on what kind of asset or what kind of communication link and what's the risk id nice
15:36
and we have for the it relevant uh technical devops engineers some nice
15:43
uh json files for example we do have a stats json that has been generated just to get
15:49
the statistic of the risks in total here critical elevated high low and their matching state in there
15:57
which they are in whether mitigated or not and we do have a json file for the details which is
16:05
actually here risks json that's basically the the whole detail set of the
16:10
identified risks but where do those risks come from that you
Risk Rules
16:15
have seen in the pdf for in the excel file in the json file so basically there are risk rules inside
16:22
fragile a built-in list of ever-growing 30-40-ish something risk rules that
16:29
are working on the parsed graph of the yaml file and have some inherent rules
16:36
accordingly due to the types of technology that have been
16:41
chosen or the types of protocol and the different other settings you provided
16:46
declaratively in the thread model yaml file so for example we have a path traversal
16:51
rule here so it's threaders written in golong the go language very simple very easy to write here we
16:59
have the metadata of the path traversal rule so we have a path traversal risk i can show you let's take a look inside this
17:06
one here path traversal risk a description impacts a little bit more mitigation stuff links to the cheat
17:13
sheet and links to the asvs chapter and this was risk was
17:18
identified one time some file server access and
17:24
that's basically the detection logic inside that method here so here we see it's a little bit of
17:30
looping a little bit of if it's not really too much complicated it's uh easy to understand and it's
17:37
just following the the graph of the objects and for file server and local file
17:42
systems it's then checking if incoming flows are basically matching the risk rule and if
17:49
it does then it's a risk that has been generated here with some likelihood and some impact and
17:56
some text for the risk details and a an id that has been generated here so that's
18:02
pretty simple and more simple rule we have another one a little bit more complex for example
18:08
unguarded direct data store access that's the one here a little bit more complex more on the
18:13
protocol side and checking things and trust boundary crossings and stuff like that so it's a little bit of
18:19
graph navigation stuff but basically it's pretty simple and of course you can also create your
18:25
own custom risk rules there's an interface for that so it's a little bit of golang
18:31
programming it's very simple you can compile that and then you can give that to your local
18:37
fragile execution with the command switch for custom risk rules
18:44
so you have a mapping where you can have custom risk rule plugins the shared object files generated from the
18:50
golang compilation of the risk rules and then these are being worked on on
18:56
the model file as well so that allows that big corporations can code or customize those risk rules and
19:03
code their own risk rules if they want to and have that way in the whole landscape
19:10
of the the corporation those risk rules applied everywhere where threads runs so you can
19:16
have your own corporate policies as code in that way that is then applied
19:21
on the decorative yaml file modeled as the thread model input
Editing YAML Files
19:27
so how about editing those thread model yaml files in your ide they that's pretty awesome and pretty
19:33
straightforward because every ide supports yaml files even in vi you can easily edit a yaml file or
19:41
it can be generated by some other kind of service whatever you like and we do even have some nice editing
19:48
support inside fragile so here there is a create editing support flag which when i
19:56
just add i'm getting two files generated schema.json and live
20:02
templates that's just a static file not depending on any model it's just the way you generate these files and you
20:08
can import those files in your ide so for example the schema
20:13
json we can use and that's the yaml schema that's basically giving us
20:20
auto completion and all those things for the ide i've imported it already
20:25
here and then i can in any model i can click here on the fragile schema
20:33
and then i do have a schema validation and auto completion so for example let's take a this jenkins build server
20:41
here as an example and make a technology build pipeline and i
20:46
have a typo here double p then basically it's flagged here
20:52
as invalid so that's not a valid one and here even i do have in my ide a validation error
20:59
and i can even have auto completion so i can just hit the control space and then i can do whatever and select
21:05
whatever kind of technology or type it a little bit and then i do have it and now it's valid again
21:11
that's pretty nice also we do have some kind of live editing support so
21:17
when you take the other file it's just a text file that has been generated and you can import that as well it's
21:23
live templates txt in your ide just a one one time effort including for
21:29
the most prominent ides some links of how you can do that
21:34
and then you can edit those templates for the base model and also for data assets
21:40
technical assets and all these communication links stuff so that you can even do some zen style coding so fragile base
21:48
model enter bam i've got the base model some title tap some name top and you can jump
21:55
through all these and here i can have a business criticality entered or whatever data assets i'm skipping a few
22:02
things data assets and then i can here have the data assets live editing support click some data
22:10
abc tab data abc tab usage then i can use control
22:17
space to have the editing support here let's say that's business and even if the confidentiality is
22:24
confidential and all these things our technical asset task asset
22:29
enter and i can give it a name some tesh asset abc
22:36
asset abc tab and type is its process you such as business used by human is
22:43
false so you see it autocompletes and taps through that and i can even have your technology
22:48
let's say it's a web server or web service rest web servers
22:54
it's not sitting on the internet so it's not the client it's exposed on the internet but not sitting on the internet
23:00
machine is a container and encryption is with symmetra key
23:05
and unfortunately not with end user identity and i can have some ratings here confidential and important
23:12
and operational and it's multi-tenant and not redundant and it
23:19
contains custom developed parts so that way i can easily get a
23:24
simple way to create those things including communication links so i can have a comp link here com link
23:33
and to abc and then i have the target id the
23:38
protocol and all these things so it's pretty straightforward to create those elements with the
23:44
live editing support including individual risk categories and their instances and also
23:52
including those risk tracking things so that's pretty simple and we do have
23:57
full model validation that way well we even have some kind of model macros which
Model Macros
24:03
are codified again in golang codified phase of to have an interactive wizard
24:09
style question back and forth with the user in order to seed a model or to modify an existing
24:17
model in a certain way adding things so when you do have certain elements in your corporation that are
24:23
often modeled in a more or less same style then you can have that codified in a thread model macro a model
24:30
macro that's working on the model like a macro modifying it with an interactive wizard style
24:36
approach so i can for example execute that at build pipeline model
24:41
here to execute with the execute model macro at build pipeline flag to add the build pipeline to
24:49
this model we are working on and we do have a few questions
24:54
whether it's basically it's about naming things and a little bit about how the build pipeline is being deployed
25:00
and it's a state machine that's coded inside the model macro so each model macro can have a
25:05
dynamic set of questions and here the default i'm accepting git and jenkins
25:11
basically the names of those build pipeline elements the artifact registry
25:16
whether we're using a code inspection platform yes are we using containerization technology
25:21
yes it's docker kubernetes as occupational orchestration runtime or can use open shifter it's
25:28
just a name and build pipeline composed components exposed to the internet
25:33
no are they used by multiple tenants yes are those encrypted
25:40
no and here i can select on which technical assets existing in the model
25:46
these deployments are being deployed on so everything that's built with that build pipeline
25:52
let's say it deploys some front-end stuff to the apache web server you see the start has
25:58
been selected also it deploys something to the erp system and it deploys something to the
26:04
marketing c ms let's say number 10 and then we're through zero and then we can even answer
26:11
the um trust boundary question so are those inside a custom trust boundary yes and
26:18
should a new one be generated yes what type is it it's let's say network uh virtual line
26:25
so it's number three and is it a push or pull based deployment pool based would be github's style pulling things
26:32
and pushing things with the classic style where you have a connection to push the deployment artifact so it's
26:38
let's say the classic one and um basically what was the owner uh it's
26:43
company xyz whoever is owning that and here we see what will be
26:48
applied that was the dry run so these things will be applied to the model adding technical assets
26:53
adding trust boundaries adding communication links and i say yes i want to execute that and
27:00
the model file has been modified so here i see the updated version a backup version of
27:05
course has been created as well if i want to switch back and i see that i do have more elements
27:10
here inside the model and if i now execute that kind of stuff
27:16
i would see new risks being generated we can just do that app work
27:25
fragile yammer and output is it's mapped in the container at work
27:33
and then we see as due to the rendering that's okay we see things have been generated i
27:39
didn't apply the verbose flag so i didn't see any details and
27:44
here i see a new data flow diagram has been generated so it looks a little bit more
27:49
complicated because the build pipeline things have been added including technical assets and data
27:55
assets like code that's flowing and we have the deployment paths being added here
28:00
and of course the risks are also reflecting that so for example we do have a few more
28:06
risks including those of the build pipeline components and also eventually depending on the
28:13
architecture we have something like uh push versus pull deployments or something like that
28:18
so that's that's a way to edit a model file interactively uh so for example x
28:24
accidental secret leak here that's a good one so a build pipeline can leak secrets and
28:30
git or docker or nexus registries if we do not have the proper counter measures in place like git
28:36
secrets um telus model other kind of things that we can use
28:41
to avoid checking secrets and so it's a new risk that's coming from the fact that we just added a build pipeline
28:46
which we did just using a model macro and you can create your own model macros of of course as well inside fragile so
28:55
that's something that you can code if you want definitely as well custom risks can be added inside the
Custom Risks
29:01
model yaml file as well so we do have an individual risk category where you can name custom risk categories and give the
29:09
meter information for the report so basically what's that that risk having for an impact and how it should
29:15
be mitigated and links to the cheat sheets and if you have something like that and you can also have instances that you
29:22
doing a manual analysis so of course when you identify
29:27
risks in the threat model workshop they should be added here as well risks that can be identified by a tool
29:34
but it's processed in the same style so you can give that risk a name and you can even map onto which kind of
29:40
technical assets this risk is applying so that means that their data loss probability calculation
29:46
things are working as expected and this kind of custom risk was added
29:51
two times so would you have two times on different technical assets this custom risks added and it's flowing
29:58
through fragile in exactly the same style as other risks but how can we track those risks the
Risk Tracking
30:05
status that they are basically in like in discussion accepted in progress mitigated stuff like that
30:12
or false positive eventually and that's also a track inside the ammo file inside the risk tracking section so
30:19
every risk either inside the ir generated excel or inside the generate pdf or inside the
30:26
generated json has a unique risk id so here i see unique risk ids that are also
30:35
inside the excel and i can just use these copy that and then paste it here as an
30:42
entry in the risk tracking and then i can give it a status of whether it's
30:47
accepted mitigated unchecked in discussion whatever kind of status you want to assign here from
30:53
those and a justification text eventually you can reference if you like optionally a
30:58
ticket like a buck tracker ticket or whatever this is being handled a date when this was checked and checked
31:05
by whom the name and this is being then generated again inside the um
31:11
with the status and here matching accordingly inside the axle and also the report is updated that
31:18
way if you regenerate so you see the bar chart here and the pie charts here uh reflecting that and you see the
31:26
impact analysis of the remaining risks so that's basically those that are not yet mitigated compared to
31:32
the initial ones this should be better obviously and you can even have wild cards so you can add wildcards if you have a
31:40
certain set like unencrypted asset a certain set of risks that you do not want on block
31:45
being visible or being shown inside the model as open because you mitigated them
31:53
or you accepted them or whatever then you can have a wildcard that's being used between those ad characters so you
32:00
can use the white card thing to have a batching of a group of risks that you want to
32:06
mitigate or want to assign a certain status to if you want to do that and you even have a model macro that is
32:13
able to if you execute that seat you with all those risk tracking entries of all the risks
Server Mode
32:19
that have been identified and threadrile also has a server mode if you want to execute it not directly
32:25
on the command line but as a rest server with an api you can have the
32:30
server attribute with a port number added fragile server running and then on that port that you defined
32:37
you've got a web page that describes the model stop and the
32:42
example files you can download the editing support and you can just upload a yaml file
32:48
and get a zip file to download then that gives you the results so then the work is executed on the server side
32:55
and this is even reflected in the api so in the api you can have that direct
33:00
analysis as well and you can even that's for the 1.1 version the extension in the api
33:06
that you can use that it's being in development currently that the server side also has a way to
33:12
store encrypted those model files you can then edit those model files using the api
33:19
listing data assets adding data assets adding technical assets changing things and you can then directly get the
33:25
results from the api as well and you can import yaml files or export that thing as a yaml file as well that's
33:32
basically exactly the same than the server that's running on run
33:37
dot threat dot io where you can experiment with that as well so for the devsec ops people that's also
DevOps
33:44
something we can use the json files that are generated automatically in a threadrill run and you can even tune it
33:51
that only the json files are generated being a little bit faster and that way you can enrich your devops
33:57
pipelines so that the threshold yaml is also checked out of the source code
34:02
repository sitting there along with the project and then it's being worked on via threadrill and then you get the
34:09
results as a json file or json files that you can process and that way you can even get
34:14
some more information and can automatically decide what to do next in the build pipeline so
34:20
it's fully working inside of uh def ops as or devsecops workflows so putting thread modeling
34:27
into devops as well and i really encourage you to use the docker container and execute it run it
34:33
play with it you get the command line you get the examples you get the docs that's pretty nice and you can also go
34:40
online to run.threadrile.io where you can have an example of playing with it or just to thread io where you then have
34:47
some more information on that and also even bigger models work quite well so this was just for the demo a
34:53
more smaller one and with a medium size one here so even more assets and connections and it
34:59
automatically been generated here that layout and we even have worked with schedule with even bigger
35:05
models so that's also feasible of handling bigger models good so for my site thank you
